{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "    print(output)\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "y = tf.multiply(2, 5)  # 10\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(y)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "y = tf.subtract(tf.cast(tf.constant(2.0), tf.int32),tf.constant(1)) \n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(y)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "x= 10\n",
    "y  =2\n",
    "z = x/y - 1'''\n",
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x, y), tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_feature = 120\n",
    "n_labels = 5\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal((n_feature, n_labels)))\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(n_features, n_labels):\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "def get_bias(n_labels):\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "def linear(inputs, weight, bias):\n",
    "    return tf.add(tf.matmul(inputs, weight), bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_features_labels(n_labels):\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "    \n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "    \n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "    return mnist_features, mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0\n",
      "Loss 8.7947\n"
     ]
    }
   ],
   "source": [
    "n_features = 784\n",
    "n_labels = 3\n",
    "\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "w = get_weights(n_features, n_labels)\n",
    "b= get_bias(n_labels)\n",
    "\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "    \n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    learning_rate = 0.08\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    _,l = sess.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features:train_features, labels:train_labels})\n",
    "    print(cross_entropy.value_index)\n",
    "\n",
    "print(\"Loss\", l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    num_batches = len(labels) // batch_size\n",
    "    last_batch = len(labels) % batch_size\n",
    "    batch = []*(num_batches + 1)\n",
    "    i = 0\n",
    "    while(i < num_batches):\n",
    "        chunk = i * batch_size\n",
    "        #print (\"i {} chunk size {}\".format(i, chunk))\n",
    "        batch.append([features[chunk:chunk+batch_size], labels[chunk:chunk+batch_size]])\n",
    "        i += 1\n",
    "    chunk = i * last_batch\n",
    "    batch.append([features[chunk:chunk+last_batch], labels[chunk:chunk+last_batch]])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    curr_cost = sess.run(cost, feed_dict={features:last_features, labels:last_labels})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={features:valid_features, labels:valid_labels})\n",
    "    print(\"Epoch {} - cost {} Accuraccy:{}\".format(epoch_i, curr_cost, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Epoch 0 - cost 13.5855073928833 Accuraccy:0.07580000162124634\n",
      "Epoch 1 - cost 12.059353828430176 Accuraccy:0.08860000222921371\n",
      "Epoch 2 - cost 10.97793960571289 Accuraccy:0.10100000351667404\n",
      "Epoch 3 - cost 10.130008697509766 Accuraccy:0.11460000276565552\n",
      "Epoch 4 - cost 9.417418479919434 Accuraccy:0.12639999389648438\n",
      "Epoch 5 - cost 8.791047096252441 Accuraccy:0.14339999854564667\n",
      "Epoch 6 - cost 8.23048210144043 Accuraccy:0.1581999957561493\n",
      "Epoch 7 - cost 7.724920272827148 Accuraccy:0.1736000031232834\n",
      "Epoch 8 - cost 7.266665458679199 Accuraccy:0.1923999935388565\n",
      "Epoch 9 - cost 6.850391864776611 Accuraccy:0.21559999883174896\n",
      "Epoch 10 - cost 6.4728007316589355 Accuraccy:0.23520000278949738\n",
      "Epoch 11 - cost 6.130922317504883 Accuraccy:0.2574000060558319\n",
      "Epoch 12 - cost 5.821261405944824 Accuraccy:0.27459999918937683\n",
      "Epoch 13 - cost 5.540400505065918 Accuraccy:0.2946000099182129\n",
      "Epoch 14 - cost 5.285472393035889 Accuraccy:0.31279999017715454\n",
      "Epoch 15 - cost 5.054114818572998 Accuraccy:0.33160001039505005\n",
      "Epoch 16 - cost 4.844160556793213 Accuraccy:0.3458000123500824\n",
      "Epoch 17 - cost 4.653397083282471 Accuraccy:0.36160001158714294\n",
      "Epoch 18 - cost 4.479538440704346 Accuraccy:0.3797999918460846\n",
      "Epoch 19 - cost 4.320399761199951 Accuraccy:0.3935999870300293\n",
      "Epoch 20 - cost 4.174066543579102 Accuraccy:0.41019999980926514\n",
      "Epoch 21 - cost 4.038940906524658 Accuraccy:0.42399999499320984\n",
      "Epoch 22 - cost 3.913707733154297 Accuraccy:0.43779999017715454\n",
      "Epoch 23 - cost 3.7972779273986816 Accuraccy:0.4453999996185303\n",
      "Epoch 24 - cost 3.6887359619140625 Accuraccy:0.45719999074935913\n",
      "Epoch 25 - cost 3.5873072147369385 Accuraccy:0.46880000829696655\n",
      "Epoch 26 - cost 3.492311954498291 Accuraccy:0.477400004863739\n",
      "Epoch 27 - cost 3.4031527042388916 Accuraccy:0.4875999987125397\n",
      "Epoch 28 - cost 3.3193018436431885 Accuraccy:0.49459999799728394\n",
      "Epoch 29 - cost 3.240290641784668 Accuraccy:0.5023999810218811\n",
      "Epoch 30 - cost 3.1657090187072754 Accuraccy:0.5112000107765198\n",
      "Epoch 31 - cost 3.0951995849609375 Accuraccy:0.5189999938011169\n",
      "Epoch 32 - cost 3.028442621231079 Accuraccy:0.5275999903678894\n",
      "Epoch 33 - cost 2.9651548862457275 Accuraccy:0.5357999801635742\n",
      "Epoch 34 - cost 2.905081272125244 Accuraccy:0.5429999828338623\n",
      "Epoch 35 - cost 2.8479909896850586 Accuraccy:0.548799991607666\n",
      "Epoch 36 - cost 2.7936723232269287 Accuraccy:0.5533999800682068\n",
      "Epoch 37 - cost 2.741931200027466 Accuraccy:0.5595999956130981\n",
      "Epoch 38 - cost 2.692594051361084 Accuraccy:0.5659999847412109\n",
      "Epoch 39 - cost 2.6454994678497314 Accuraccy:0.571399986743927\n",
      "Epoch 40 - cost 2.60050106048584 Accuraccy:0.5771999955177307\n",
      "Epoch 41 - cost 2.557464599609375 Accuraccy:0.5820000171661377\n",
      "Epoch 42 - cost 2.516268730163574 Accuraccy:0.5878000259399414\n",
      "Epoch 43 - cost 2.4768006801605225 Accuraccy:0.5942000150680542\n",
      "Epoch 44 - cost 2.4389567375183105 Accuraccy:0.5996000170707703\n",
      "Epoch 45 - cost 2.4026410579681396 Accuraccy:0.6039999723434448\n",
      "Epoch 46 - cost 2.3677659034729004 Accuraccy:0.609000027179718\n",
      "Epoch 47 - cost 2.334247589111328 Accuraccy:0.6133999824523926\n",
      "Epoch 48 - cost 2.302011013031006 Accuraccy:0.6177999973297119\n",
      "Epoch 49 - cost 2.2709860801696777 Accuraccy:0.6237999796867371\n",
      "Epoch 50 - cost 2.2411062717437744 Accuraccy:0.6281999945640564\n",
      "Epoch 51 - cost 2.212310552597046 Accuraccy:0.6331999897956848\n",
      "Epoch 52 - cost 2.1845405101776123 Accuraccy:0.6380000114440918\n",
      "Epoch 53 - cost 2.1577444076538086 Accuraccy:0.6425999999046326\n",
      "Epoch 54 - cost 2.131873846054077 Accuraccy:0.6452000141143799\n",
      "Epoch 55 - cost 2.1068801879882812 Accuraccy:0.649399995803833\n",
      "Epoch 56 - cost 2.0827221870422363 Accuraccy:0.6534000039100647\n",
      "Epoch 57 - cost 2.059359073638916 Accuraccy:0.6570000052452087\n",
      "Epoch 58 - cost 2.0367534160614014 Accuraccy:0.6597999930381775\n",
      "Epoch 59 - cost 2.0148704051971436 Accuraccy:0.6615999937057495\n",
      "Epoch 60 - cost 1.9936766624450684 Accuraccy:0.6647999882698059\n",
      "Epoch 61 - cost 1.9731414318084717 Accuraccy:0.6690000295639038\n",
      "Epoch 62 - cost 1.9532347917556763 Accuraccy:0.6736000180244446\n",
      "Epoch 63 - cost 1.9339299201965332 Accuraccy:0.676800012588501\n",
      "Epoch 64 - cost 1.915200114250183 Accuraccy:0.6779999732971191\n",
      "Epoch 65 - cost 1.8970211744308472 Accuraccy:0.6809999942779541\n",
      "Epoch 66 - cost 1.8793705701828003 Accuraccy:0.6836000084877014\n",
      "Epoch 67 - cost 1.8622252941131592 Accuraccy:0.6851999759674072\n",
      "Epoch 68 - cost 1.845564603805542 Accuraccy:0.6876000165939331\n",
      "Epoch 69 - cost 1.8293681144714355 Accuraccy:0.6899999976158142\n",
      "Epoch 70 - cost 1.8136166334152222 Accuraccy:0.6931999921798706\n",
      "Epoch 71 - cost 1.7982937097549438 Accuraccy:0.694599986076355\n",
      "Epoch 72 - cost 1.7833797931671143 Accuraccy:0.6959999799728394\n",
      "Epoch 73 - cost 1.7688603401184082 Accuraccy:0.6980000138282776\n",
      "Epoch 74 - cost 1.754719614982605 Accuraccy:0.701200008392334\n",
      "Epoch 75 - cost 1.7409424781799316 Accuraccy:0.7035999894142151\n",
      "Epoch 76 - cost 1.7275149822235107 Accuraccy:0.7049999833106995\n",
      "Epoch 77 - cost 1.714423656463623 Accuraccy:0.7070000171661377\n",
      "Epoch 78 - cost 1.7016552686691284 Accuraccy:0.7093999981880188\n",
      "Epoch 79 - cost 1.6891975402832031 Accuraccy:0.7103999853134155\n",
      "Epoch 80 - cost 1.6770386695861816 Accuraccy:0.7124000191688538\n",
      "Epoch 81 - cost 1.6651676893234253 Accuraccy:0.7152000069618225\n",
      "Epoch 82 - cost 1.6535747051239014 Accuraccy:0.7182000279426575\n",
      "Epoch 83 - cost 1.6422486305236816 Accuraccy:0.7193999886512756\n",
      "Epoch 84 - cost 1.631179928779602 Accuraccy:0.7214000225067139\n",
      "Epoch 85 - cost 1.6203598976135254 Accuraccy:0.7232000231742859\n",
      "Epoch 86 - cost 1.6097807884216309 Accuraccy:0.723800003528595\n",
      "Epoch 87 - cost 1.5994316339492798 Accuraccy:0.7264000177383423\n",
      "Epoch 88 - cost 1.5893065929412842 Accuraccy:0.727400004863739\n",
      "Epoch 89 - cost 1.5793967247009277 Accuraccy:0.7289999723434448\n",
      "Epoch 90 - cost 1.5696955919265747 Accuraccy:0.7301999926567078\n",
      "Epoch 91 - cost 1.5601962804794312 Accuraccy:0.7315999865531921\n",
      "Epoch 92 - cost 1.550891399383545 Accuraccy:0.7351999878883362\n",
      "Epoch 93 - cost 1.5417747497558594 Accuraccy:0.7368000149726868\n",
      "Epoch 94 - cost 1.5328407287597656 Accuraccy:0.7378000020980835\n",
      "Epoch 95 - cost 1.5240836143493652 Accuraccy:0.7390000224113464\n",
      "Epoch 96 - cost 1.5154964923858643 Accuraccy:0.7408000230789185\n",
      "Epoch 97 - cost 1.507075548171997 Accuraccy:0.7422000169754028\n",
      "Epoch 98 - cost 1.4988150596618652 Accuraccy:0.7429999709129333\n",
      "Epoch 99 - cost 1.4907115697860718 Accuraccy:0.7437999844551086\n",
      "Test Accuracy 0.7498000264167786\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "lr = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "epochs = 100\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.truncated_normal([n_classes]))\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits=logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "batch_size = 128\n",
    "batch = batches(batch_size, train_features, train_labels)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        for batch_features, batch_labels in batch:\n",
    "            sess.run(optimizer, feed_dict={features:batch_features, labels:batch_labels, learning_rate:lr})\n",
    "\n",
    "        epoch_stats(i, sess, batch_features, batch_labels)\n",
    "    test_acc = sess.run(accuracy, feed_dict={features:test_features, labels:test_labels})\n",
    "print(\"Test Accuracy {}\".format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
