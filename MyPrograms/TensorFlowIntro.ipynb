{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "    print(output)\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "y = tf.multiply(2, 5)  # 10\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(y)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "y = tf.subtract(tf.cast(tf.constant(2.0), tf.int32),tf.constant(1)) \n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(y)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "x= 10\n",
    "y  =2\n",
    "z = x/y - 1'''\n",
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x, y), tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_feature = 120\n",
    "n_labels = 5\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal((n_feature, n_labels)))\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(n_features, n_labels):\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "def get_bias(n_labels):\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "def linear(inputs, weight, bias):\n",
    "    return tf.add(tf.matmul(inputs, weight), bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_features_labels(n_labels):\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "    \n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "    \n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "    return mnist_features, mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0\n",
      "Loss 8.7947\n"
     ]
    }
   ],
   "source": [
    "n_features = 784\n",
    "n_labels = 3\n",
    "\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "w = get_weights(n_features, n_labels)\n",
    "b= get_bias(n_labels)\n",
    "\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "    \n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    learning_rate = 0.08\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    _,l = sess.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features:train_features, labels:train_labels})\n",
    "    print(cross_entropy.value_index)\n",
    "\n",
    "print(\"Loss\", l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    num_batches = len(labels) // batch_size\n",
    "    last_batch = len(labels) % batch_size\n",
    "    batch = []*(num_batches + 1)\n",
    "    i = 0\n",
    "    while(i < num_batches):\n",
    "        chunk = i * batch_size\n",
    "        #print (\"i {} chunk size {}\".format(i, chunk))\n",
    "        batch.append([features[chunk:chunk+batch_size], labels[chunk:chunk+batch_size]])\n",
    "        i += 1\n",
    "    chunk = i * last_batch\n",
    "    batch.append([features[chunk:chunk+last_batch], labels[chunk:chunk+last_batch]])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0 chunk size 0\n",
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F21', 'F22', 'F23', 'F24']], [['L21', 'L22']]]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epochs_stat(epoch_i, sess, last_features, last_labels):\n",
    "    sess.run(cost, feed_dict={features:last_features. labels:last_labels})\n",
    "    valid_acc = sess.run(accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy 0.06499999761581421\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.truncated_normal([n_classes]))\n",
    "\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits=logits))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    batch = batches(batch_size, train_features, train_labels)\n",
    "    \n",
    "    for batch_features, batch_labels in batch:\n",
    "        sess.run(optimizer, feed_dict={features:batch_features, labels:batch_labels})\n",
    "\n",
    "    test_acc = sess.run(accuracy, feed_dict={features:test_features, labels:test_labels})\n",
    "print(\"Test Accuracy {}\".format(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
